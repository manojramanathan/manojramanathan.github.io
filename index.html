<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Home | Manoj Ramanathan</title>
    <link rel="stylesheet" href="styles.css">
    <link
    href='../fonts.googleapis.com/cssefa4.css?family=Lato:300,400,700|Nunito:300,700' rel='stylesheet' type='text/css'>
    <script src="../code.jquery.com/jquery-1.11.3.min.js"></script>
    <script src="../code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
    <script src="index.js"></script>

  </head>
  <body>
    <div id="mainContent">

        <h1>MANOJ <span style="font-weight:300;">RAMANATHAN</span></h1>
    </div>
    <ul id="nav">
	<li>mramanathan@ntu.edu.sg | </li>

  <li><a href="https://scholar.google.com.sg/citations?user=A_y583kAAAAJ&hl=en" target="_blank">Google Scholar</a></li>
	<li><a href="files/Manoj_Ramanathan_Research_Resume.pdf" target="_blank">CV</a></li>

    </ul>
    <div id="main">



<p><img src="Manoj.jpg" width="35%" style="margin-left: 50px; float:right; margin-bottom: 50px;" />
<span style="font-weight: 400;">
Currenty, I am post-doctoral research fellow in Rehabilitative Research Institute of Singapore, NTU working on the development of vision based perception module for an exo skeleton to be used by stroke patients. 
I have interesting mix of research and industrial experience. During my first post doctorate, I had the opportunity to handle and work on the AI platform of Nadine social humanoid robot in IMI, NTU, Singapore. I carried over the experience and passion over to a startup industry, <a href="https://dex-lab.com/" target="_blank">DEX-Lab AI</a> AI, Singapore where I worked as an AI Research Scientist.
I was fortunate to have been co-advised by <a href="https://en.wikipedia.org/wiki/Nadia_Magnenat_Thalmann" target="_blank">Nadia Magnenat Thalmann</a> and <a href="https://en.wikipedia.org/wiki/Daniel_Thalmann" target="_blank">Daniel Thalmann</a> during both these stints.
<!-- Previously, I worked as an AI research scientist at  <a href="https://dex-lab.com/" target="_blank">DEX-Lab AI</a>, Singapore, fortunate to be coadvised by <a href="https://en.wikipedia.org/wiki/Nadia_Magnenat_Thalmann" target="_blank">Nadia Magnenat Thalmann</a> and <a href="https://en.wikipedia.org/wiki/Daniel_Thalmann" target="_blank">Daniel Thalmann</a>. -->
<br /> <br />I graduated from School of Electrical and Electronics Engineering, NTU, Singapore with a PhD specialized in Computer Vision. Specifically, my PhD thesis is titled "Pose-invariant Action Recognition for Automated Behavior Analysis". (Supervised by Assoc. Prof. Teoh Eam Khwang and Dr. Yau Wei-Yun)
	My research interests span from Computer Vision, Artificial Intelligence, Social Robotics and Virtual Character embodiments, Human Computer Interaction (HCI).
</span></p>

<h2 id="papers">Selected Papers</h2>

<!-- <h4 id="denotes-equal-contribution">* denotes equal contribution</h4> -->
<a id="selectedPapers">Show all</a><br />

<p><a href="https://dr.ntu.edu.sg/bitstream/10220/24233/1/Human%20action%20recognition%20with%20video%20data%20%20Research%20and%20evaluation%20challenges.pdf" target="_blank" class="papertitle"> Human action recognition with video data: research and evaluation challenges</a>
<br /> Manoj Ramanathan, Wei-Yun Yau, Eam Khwang Teoh
<br /><strong><em>Oral presentation</em></strong>, IEEE Transactions on Human-Machine Systems (2014).</p>

<p><a class="papertitle" href="https://www.researchgate.net/profile/Nidhi_Mishra14/publication/333706136_Nadine_Humanoid_Social_Robotics_Platform/links/5e83fff8a6fdcca789e59091/Nadine-Humanoid-Social-Robotics-Platform.pdf" target="_blank">Nadine Humanoid Social Robotics Platform</a>
<br /> Manoj Ramanathan, Nidhi Mishra, Nadia Magnenat Thalmann,
<br /><strong><em>Oral presentation</em></strong>, Computer Graphics International Conference (2019)</p>

<p><a class="papertitle" href="https://arxiv.org/pdf/1905.08937.pdf" target="_blank">Can a Humanoid Robot be part of the Organizational Work Force? A User Study leveraging on Sentiment Analysis</a>
<br /> Nidhi Mishra, Manoj Ramanathan, Ranjan Satapathy, Erik Cambria, Nadia Magnenat-Thalmann,
<br /><strong><em>Oral presentation</em></strong>, IEEE RO-MAN (2019)</p>

<p><a class="papertitle" href="https://link.springer.com/chapter/10.1007/978-3-030-71002-6_14" target="_blank">Survey of Speechless Interaction Techniques in Social Robotics</a>
<br /> Manoj Ramanathan, Ranjan Satapathy, Nadia Magnenat Thalmann,
<br /><strong><em>Book Chapter</em></strong> in Intelligent Scene Modelling and Human-Computer Interaction, Human Computer Interaction Series (Eds. Nadia Magnenat Thalmann, Jian J Zhang, Manoj Ramanathan, Daniel Thalmann)(2021) </p>

<p><a class="papertitle" href="https://dr.ntu.edu.sg/bitstream/10356/102468/1/Combining%20Pose-Invariant%20Kinematic%20Features%20and%20Object%20Context%20Features%20for%20RGB-D%20Action%20Recognition.pdf" target="_blank">Combining pose-invariant kinematic features and object context features for RGB-D action recognition</a>
<br /> Manoj Ramanathan, Jaroslaw Kochanowicz, Nadia Magnenat Thalmann,
<br /><strong><em>Oral presentation</em></strong>, International Journal of Machine Learning and Computing (2019)</p>

<p><a class="papertitle" href="http://www.inderscience.com/offer.php?id=99014" target="_blank">Mutually reinforcing motion-pose framework for pose invariant action recognition</a>
<br /> Manoj Ramanathan, Wei-Yun Yau, Nadia Magnenat Thalmann, Eam Khwang Teoh,
<br /><strong><em>Oral presentation</em></strong>, International Journal of Biometrics (2019)</p>
<!-- http://imi.ntu.edu.sg/NewsEvents/Events/PastSeminars/Documents/22_November_2016/Manoj_Ramanathan_22_November_2016.pdf -->
<p><a class="papertitle" href="https://dr.ntu.edu.sg/bitstream/10356/138068/2/18_Pose-Invariant%20Kinematic%20Features%20for%20Action%20Recognition.pdf" target="_blank">Pose-invariant kinematic features for action recognition</a>
<br /> Manoj Ramanathan, Wei-Yun Yau, Eam Khwang Teoh, Nadia Magnenat Thalmann,
<br /><strong><em>Oral presentation</em></strong>, IEEE APSIPA ASC (2017).</p>

<p><a class="papertitle" href="https://dl.acm.org/doi/abs/10.1145/3134472.3134513" target="_blank">Real humans with virtual humans and social robots interactions (HCI)</a>
<br /> Daniel Thalmann, Nadia Magnenat Thalmann, Manoj Ramanathan (2017)
<br /><strong><em>Oral presentation</em></strong>, SIGGRAPH Asia 2017.</p>

<p><a class="papertitle" href="https://dr.ntu.edu.sg/bitstream/10356/70099/1/Thesis.pdf" target="_blank">Pose-invariant action recognition for automated behaviour analysis</a>
<br /> Manoj Ramanathan (2017),
<br /><strong><em>Thesis</em></strong>, Nanyang Technological University (NTU), Singapore (2017)</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/7838651/" target="_blank" class="papertitle">Improving human body part detection using deep learning and motion consistency</a>
<br /> Manoj Ramanathan, Wei-Yun Yau, Eam Khwang Teoh,
<br /><strong><em>Oral presentation</em></strong>, International Conference on Control, Automation, Robotics and Vision (ICARCV) 2016.</p>

<p><a href="https://dl.acm.org/doi/abs/10.1145/2974804.2980480" target="_blank" class="papertitle">Human posture detection using H-ELM body part and whole person detectors for human-robot interaction</a>
<br /> Manoj Ramanathan, Wei Yun Yau, Eam Khwang Teoh
<br />Proceedings of the Fourth International Conference on Human Agent Interaction (2016).</p>

<p><a href="https://oar.a-star.edu.sg/jspui/bitstream/123456789/1662/1/Human%20Body%20Part%20Detection%20Using%20Likelihood%20Score%20Computations_PID3387567.pdf" target="_blank" class="papertitle">Human body part detection using likelihood score computations</a>
<br /> Manoj Ramanathan, Wei-Yun Yau, Eam Khwang Teoh
<br /><strong><em>Oral presentation</em></strong>, Computational Intelligence in Biometrics and Identity Management (CIBIM), 2014.</p>

<p class="ns"><a href="https://www.vi-mm.eu/wp-content/uploads/2016/12/Nadine-case-study.pdf" target="_blank" class="papertitle">Meet Nadine, one of the worldâ€™s most human-like robots</a>
<br /> Evangelia Baka, Manoj Ramanathan, Nidhi Mishra, Nadia Magnenat Thalmann
<br /><strong><em>Oral presentation</em></strong>, ICDE (2018).</p>




<h2 id="short-papersmiscellanea">Statements</h2>
<p></p>
<ul class="projectlist">
  <li><label for="proj-10"> &gt; <a href="files/Research_Statement.pdf" target="_blank">Research Statement</a></label></li>
  <li><label for="proj-11"> &gt; <a href="files/Teaching_Statement.pdf" target="_blank">Teaching Statement</a></label></li>
  <li><label for="proj-12"> &gt; <a href="files/Diversity_Statement.pdf" target="_blank">Diversity Statement</a></label></li>
</ul>




<h2 id="personal-projects">Research projects so far</h2>
<p>This section provides a list of my research projects so far.</p>
<ul class="projectlist">
<li>
 <label for="proj-3"> &gt; Chloe: A customizable virtual human</label>
 <input type="checkbox" id="proj-3" /> <p><img src="images/BuzzerSystem.jpg" width="50px" />Chloe is fully functional, customizable virtual human from Dex-lab. She can be customized to work as Office receptionist, Mall concierge, Elderly home assistant etc. Her AI platform can be customized and includes several vision, NLP, affective, memory models that allow her to interact and behave like a human. Please refer to <a href="https://dex-lab.com/" target="_blank">DEX-Lab AI</a> for more details.</p></li>  
<li>
 <label for="proj-2"> &gt; Dexie: Realistic Humanoid Robot</label>
 <input type="checkbox" id="proj-2" /> <p><img src="images/3DModeling.jpg" width="50px" />Dexie is a social humanoid robot developed in Dex-lab. I have worked on the complete AI backend that controls her actions and behavior. Chloe and Dexie share a similar AI backend which can generate behaviors according to agent using it (virtual/physical). Dexie can be customized to work in different roles and organizations with several functionalities. Please refer to <a href="https://dex-lab.com/" target="_blank">DEX-Lab AI</a> for more details.</p></li>
<li>
 <label for="proj-4"> &gt; Nadine Social Robotics Platform</label>
 <input type="checkbox" id="proj-4" /> <p><img src="Nadine.jpg" width="50px" /><a href="https://en.wikipedia.org/wiki/Nadine_Social_Robot" target="_blank">Nadine</a> is one of the most realistic live-sized social humanoid robot with artificial skin, articulated hands and fingers (from IMI, NTU). I have worked on the development of AI backend (perception-processing-interaction of stimuli) of the robot since 2016. During this time, we have included several state of the art vision models (face, action recognition etc), NLP models (sentiment analysis, chatbots etc), emotion and memory models. She has also worked as receptionist in AIA, Singapore, caretaker in an Elderly home in Singapore. Please refer to my publilcations above on her for more details. Please see <a href="https://www.youtube.com/user/IMINTUsg/featured" target="_blank">IMI, NTU, Singapore</a> for Nadine's videos.</p></li>
 <li>
  <label for="proj-5"> &gt; Pose-invariant Action Recognition for Automated Behavior Analysis</label>
  <input type="checkbox" id="proj-5" /> <p><img src="images/Falcon.jpg" width="50px" />This was my PhD topic where I was first introduced to computer vision topics. I worked detecting actions in a pose-invariant manner from 2D action videos in unconstrained environment. I mainly tackled the problem of View-invariance and occlusion resistant RGB action recognition by proposing a Mutual reinforcing motion-pose framework inspired by control system architecture. Please refer to my above publication for more details.</p></li>
</ul>






<script>
// Google analytics
</script>

    </div>
  </body>

</html>
